<!DOCTYPE html>
<html lang="en">
<head>
    <meta charset="UTF-8">
    <meta http-equiv="X-UA-Compatible" content="IE=edge">
    <meta name="viewport" content="width=device-width, initial-scale=1.0">
    <title>Superstore Dataset - Predicting Profit with Regression and Decision Tree Models</title>
    <style>
        body {
            font-family: Arial, sans-serif;
            line-height: 1.6;
            margin: 20px;
        }
        .section {
            margin-bottom: 40px;
        }
        h2 {
            color: #2C3E50;
        }
        p {
            margin-bottom: 15px;
        }
        pre {
            background-color: #f4f4f4;
            padding: 10px;
            border-left: 3px solid #ddd;
            overflow-x: auto;
        }
        img {
            display: block;
            margin: 20px auto;
            max-width: 100%;
        }
    </style>
</head>
<body>

    <header>
        <h1>Superstore Dataset: Predicting Profit with Regression and Decision Tree Models</h1>
    </header>

    <section class="section">
        <h2>Introduction</h2>
        <p>The "Superstore Dataset" was created specifically for educational purposes, providing valuable insights into retail data. The dataset is widely used for data visualization and analysis tasks. In this project, we aim to explore patterns in sales and profit by building various machine learning models. Specifically, we predict profit using regression techniques and a decision tree classifier for categorical profit analysis. The dataset contains features like product category, customer segments, sales, and discounts, which are key drivers for predicting profitability.</p>
        <p>The dataset can be found on <a href="https://www.kaggle.com/datasets/vivek468/superstore-dataset-final" target="_blank">Kaggle</a>
            the full code is  <a href="https://github.com/GriselQ23/Data-Mining-project/tree/main" target="_blank">here</a> 
            You can find the report <a href="images/Data_Mining_Project Grisel Quispe.pdf " download style="color: blue;">here</a>
            .</p>
    </section>

    <section class="section">
        <h2>Dataset Understanding</h2>
        <p>The dataset consists of 21 features and 9994 instances. It includes both categorical and numerical variables, such as:</p>
        <ul>
            <li><strong>Categorical:</strong> Ship Mode, Customer ID, Segment, Country, City, State, Region, Category, Sub-Category</li>
            <li><strong>Numerical:</strong> Sales, Quantity, Discount, Profit</li>
        </ul>
        <p>We cleaned the dataset by handling missing values, removing outliers, and transforming some features for better model performance. We also performed data mining techniques, including association rule mining and clustering, to find hidden patterns in customer purchase behavior.</p>
    </section>

    <section class="section">
        <h2>1. Regression Models</h2>
        <p>We applied multiple regression models (Linear, Lasso, Ridge, and ElasticNet) to predict the profit of sales transactions. The dataset was split into 80% training and 20% testing. For each model, we predicted the profit on the test set and evaluated the model's performance using various metrics such as R-squared, Mean Squared Error (MSE), and Root Mean Squared Error (RMSE).</p>
        
        <img src="file-path-to-your-regression-plot.png" alt="Predicted vs Actual for Regression Models" width="600">
        
        <p>The above scatter plot shows the actual vs predicted profit values for the regression models. Ideally, points should lie along the red dashed line, which represents perfect predictions. ElasticNet and Ridge models performed best in reducing error and avoiding overfitting.</p>

        <h3>Code Example:</h3>
        <pre>
            # Linear regression
            lm_model <- lm(Profit ~ ., data = X_train)
            lm_predictions <- predict(lm_model, newdata = X_test)
            lm_metrics <- calculate_metrics(y_test, lm_predictions)
            
            # Lasso regression
            lasso_model <- train(
                x = as.matrix(X_train),
                y = y_train,
                method = "glmnet",
                trControl = trainControl("cv"),
                tuneGrid = expand.grid(alpha = 1, lambda = seq(0.001, 1, by = 0.01))
            )
            lasso_predictions <- predict(lasso_model, newdata = as.matrix(X_test))
            lasso_metrics <- calculate_metrics(y_test, lasso_predictions)
        </pre>
    </section>

    <section class="section">
        <h2>2. Decision Tree Model</h2>
        <p>To further analyze profit prediction, we converted the profit into a categorical variable with three classes. We then built a decision tree classifier to predict which category each transaction falls into based on various features such as customer segment, product category, and region.</p>
        
        <img src="file-path-to-your-decision-tree-plot.png" alt="Decision Tree for Profit Prediction" width="600">
        
        <p>The decision tree model splits the data at important feature points to classify profit into different categories. As shown in the diagram, features like customer segment and product sub-category were the most significant in classifying profit into low, medium, and high categories.</p>

        <h3>Code Example:</h3>
        <pre>
            # Build decision tree
            profit_tree <- rpart(Profit_Category ~ ., data = df, method = "class")
            
            # Plot the tree
            rpart.plot(profit_tree, main = "Decision Tree for Profit Categories")
            
            # Evaluate model
            predictions <- predict(profit_tree, test_set, type = "class")
            cm <- confusionMatrix(predictions, test_set$Profit_Category)
            print(cm)
        </pre>
    </section>

    <section class="section">
        <h2>3. Clustering & Apriori Analysis</h2>
        <p>To gain deeper insights into customer behavior, we used K-means clustering to group customers based on their sales and profit. Additionally, we applied the Apriori algorithm to identify patterns in customer segments, regions, and product categories.</p>

        <h3>Code Example:</h3>
        <pre>
            # K-means clustering on sales and profit
            set.seed(123)
            kmeans_result <- kmeans(sales_profit_data, centers = 3)
            
            # Add cluster labels
            filtered_data$Cluster <- kmeans_result$cluster
            
            # Apriori algorithm for pattern discovery
            rules <- apriori(transactions, parameter = list(support = 0.2, confidence = 0.8))
            inspect(rules)
        </pre>
    </section>

    <section class="section">
        <h2>Evaluation Metrics</h2>
        <p>For regression models, we evaluated performance using:</p>
        <ul>
            <li>R-squared</li>
            <li>Mean Squared Error (MSE)</li>
            <li>Mean Absolute Error (MAE)</li>
            <li>Root Mean Squared Error (RMSE)</li>
        </ul>
        <p>For the decision tree classifier, we computed:</p>
        <ul>
            <li>Accuracy</li>
            <li>Precision</li>
            <li>Recall</li>
        </ul>
    </section>

</body>
</html>

